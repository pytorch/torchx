{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx[kfp]\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/heads/main.tar.gz\n!tar xf main.tar.gz --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTrainer Example\n=============================================\n\nThis is an example TorchX app that uses PyTorch Lightning to train a model.\n\nThis app only uses standard OSS libraries and has no runtime torchx\ndependencies. For saving and loading data and models it uses fsspec which makes\nthe app agnostic to the environment it's running in.\n\nUsage\n---------\n\nTo run the trainer locally as a ddp application with 1 node and 2 workers-per-node (world size = 2):\n\n.. code:: shell-session\n\n  $ torchx run -s local_cwd dist.ddp\n     -j 1x2\n     --script ./lightning/train.py\n     --\n     --epochs=1\n     --output_path=/tmp/torchx/train\n     --log_path=/tmp/torchx/logs\n     --skip_export\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>``--`` is used to delimit between component (``dist.ddp``) and\n          application arguments.</p></div>\n\nUse the ``--help`` option to see the full list of application options:\n\n.. code:: shell-session\n\n  $ torchx run -s local_cwd dist.ddp -j 1x1 --script ./lightning/train.py -- --help\n\nWhich is effectively the same as ``./train.py --help``. To run on a remote scheduler,\nspecify the scheduler with the ``-s`` option. Depending on the type of remote scheduler\nyou may have to pass additional scheduler cfgs with the ``-cfg`` option.\nSee `Quickstart:Remote Schedulers` for more details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport os\nimport sys\nimport tempfile\nfrom typing import List, Optional\n\nimport pytorch_lightning as pl\nimport torch\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch.distributed.elastic.multiprocessing import errors\nfrom torchx.examples.apps.lightning.data import (\n    create_random_data,\n    download_data,\n    TinyImageNetDataModule,\n)\nfrom torchx.examples.apps.lightning.model import (\n    export_inference_model,\n    TinyImageNetModel,\n)\nfrom torchx.examples.apps.lightning.profiler import SimpleLoggingProfiler\n\n\n# ensure data and module are on the path\nsys.path.append(\".\")\n\n\ndef parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"pytorch lightning TorchX example app\")\n    parser.add_argument(\n        \"--epochs\", type=int, default=3, help=\"number of epochs to train\"\n    )\n    parser.add_argument(\"--lr\", type=float, help=\"learning rate\")\n    parser.add_argument(\n        \"--batch_size\", type=int, default=32, help=\"batch size to use for training\"\n    )\n    parser.add_argument(\n        \"--num_samples\",\n        type=int,\n        default=32,\n        help=\"number of samples in the dataset\",\n    )\n    parser.add_argument(\n        \"--data_path\",\n        type=str,\n        help=\"path to load the training data from, if not provided, random data will be generated\",\n    )\n    parser.add_argument(\"--skip_export\", action=\"store_true\")\n    parser.add_argument(\"--load_path\", type=str, help=\"checkpoint path to load from\")\n    parser.add_argument(\n        \"--output_path\",\n        type=str,\n        help=\"path to place checkpoints and model outputs, if not specified, checkpoints are not saved\",\n    )\n    parser.add_argument(\n        \"--log_path\",\n        type=str,\n        help=\"path to place the tensorboard logs\",\n        default=\"/tmp\",\n    )\n    parser.add_argument(\n        \"--layers\",\n        nargs=\"+\",\n        type=int,\n        help=\"the MLP hidden layers and sizes, used for neural architecture search\",\n    )\n    return parser.parse_args(argv)\n\n\ndef get_model_checkpoint(args: argparse.Namespace) -> Optional[ModelCheckpoint]:\n    if not args.output_path:\n        return None\n    # Note: It is important that each rank behaves the same.\n    # All of the ranks, or none of them should return ModelCheckpoint\n    # Otherwise, there will be deadlock for distributed training\n    return ModelCheckpoint(\n        monitor=\"train_loss\",\n        dirpath=args.output_path,\n        save_last=True,\n    )\n\n\n@errors.record\ndef main(argv: List[str]) -> None:\n    with tempfile.TemporaryDirectory() as tmpdir:\n        args = parse_args(argv)\n\n        # Init our model\n        model = TinyImageNetModel(args.layers)\n        print(model)\n\n        # Download and setup the data module\n        if not args.data_path:\n            data_path = os.path.join(tmpdir, \"data\")\n            os.makedirs(data_path)\n            create_random_data(data_path, args.num_samples)\n        else:\n            data_path = download_data(args.data_path, tmpdir)\n\n        data = TinyImageNetDataModule(\n            data_dir=data_path,\n            batch_size=args.batch_size,\n            num_samples=args.num_samples,\n        )\n\n        # Setup model checkpointing\n        checkpoint_callback = get_model_checkpoint(args)\n        callbacks = []\n        if checkpoint_callback:\n            callbacks.append(checkpoint_callback)\n        if args.load_path:\n            print(f\"loading checkpoint: {args.load_path}...\")\n            model.load_from_checkpoint(checkpoint_path=args.load_path)\n\n        logger = TensorBoardLogger(\n            save_dir=args.log_path, version=1, name=\"lightning_logs\"\n        )\n        # Initialize a trainer\n        trainer = pl.Trainer(\n            num_nodes=int(os.environ.get(\"GROUP_WORLD_SIZE\", 1)),\n            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n            devices=int(os.environ.get(\"LOCAL_WORLD_SIZE\", 1)),\n            strategy=\"ddp\",\n            logger=logger,\n            max_epochs=args.epochs,\n            callbacks=callbacks,\n            profiler=SimpleLoggingProfiler(logger),\n        )\n\n        # Train the model \u26a1\n        trainer.fit(model, data)\n        print(\n            f\"train acc: {model.train_acc.compute()}, val acc: {model.val_acc.compute()}\"\n        )\n\n        rank = int(os.environ.get(\"RANK\", 0))\n        if rank == 0 and not args.skip_export and args.output_path:\n            # Export the inference model\n            export_inference_model(model, args.output_path, tmpdir)\n\n\nif __name__ == \"__main__\" and \"NOTEBOOK\" not in globals():\n    main(sys.argv[1:])\n\n\n# sphinx_gallery_thumbnail_path = '_static/img/gallery-app.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}