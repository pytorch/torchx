{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx[kfp]\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/tags/v0.6.0.tar.gz\n!tar xf v0.6.0.tar.gz --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTiny ImageNet Model\n====================\n\nThis is a toy model for doing regression on the tiny imagenet dataset. It's used\nby the apps in the same folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path\nimport subprocess\nfrom typing import List, Optional, Tuple\n\nimport fsspec\nimport pytorch_lightning as pl\nimport torch\nimport torch.jit\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy\nfrom torchvision.models.resnet import BasicBlock, ResNet\n\n\nclass TinyImageNetModel(pl.LightningModule):\n    \"\"\"\n    An very simple linear model for the tiny image net dataset.\n    \"\"\"\n\n    def __init__(\n        self, layer_sizes: Optional[List[int]] = None, lr: Optional[float] = None\n    ) -> None:\n        super().__init__()\n\n        if not layer_sizes:\n            layer_sizes = [1, 1, 1, 1]\n\n        self.lr: float = lr or 0.001\n\n        # We use the torchvision resnet model with some small tweaks to match\n        # TinyImageNet.\n        m = ResNet(BasicBlock, layer_sizes)\n        m.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n        m.fc.out_features = 200\n        self.model: ResNet = m\n\n        self.train_acc = Accuracy()\n        self.val_acc = Accuracy()\n\n    # pyre-fixme[14]\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    # pyre-fixme[14]\n    def training_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -> torch.Tensor:\n        return self._step(\"train\", self.train_acc, batch, batch_idx)\n\n    # pyre-fixme[14]\n    def validation_step(\n        self, val_batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n    ) -> torch.Tensor:\n        return self._step(\"val\", self.val_acc, val_batch, batch_idx)\n\n    def _step(\n        self,\n        step_name: str,\n        acc_metric: Accuracy,\n        batch: Tuple[torch.Tensor, torch.Tensor],\n        batch_idx: int,\n    ) -> torch.Tensor:\n        x, y = batch\n        y_pred = self(x)\n        loss = F.cross_entropy(y_pred, y)\n        self.log(f\"{step_name}_loss\", loss)\n        acc_metric(y_pred, y)\n        self.log(f\"{step_name}_acc\", acc_metric.compute())\n        return loss\n\n    # pyre-fixme[3]: TODO(aivanou): Figure out why oss pyre can identify type but fb cannot.\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n\n\ndef export_inference_model(\n    model: TinyImageNetModel, out_path: str, tmpdir: str\n) -> None:\n    \"\"\"\n    export_inference_model uses TorchScript JIT to serialize the\n    TinyImageNetModel into a standalone file that can be used during inference.\n    TorchServe can also handle interpreted models with just the model.py file if\n    your model can't be JITed.\n    \"\"\"\n\n    print(\"exporting inference model\")\n    jit_path = os.path.join(tmpdir, \"model_jit.pt\")\n    jitted = torch.jit.script(model)\n    print(f\"saving JIT model to {jit_path}\")\n    torch.jit.save(jitted, jit_path)\n\n    model_name = \"tiny_image_net\"\n\n    mar_path = os.path.join(tmpdir, f\"{model_name}.mar\")\n    print(f\"creating model archive at {mar_path}\")\n    subprocess.run(\n        [\n            \"torch-model-archiver\",\n            \"--model-name\",\n            \"tiny_image_net\",\n            \"--handler\",\n            \"torchx/examples/apps/lightning/handler/handler.py\",\n            \"--version\",\n            \"1\",\n            \"--serialized-file\",\n            jit_path,\n            \"--export-path\",\n            tmpdir,\n        ],\n        check=True,\n    )\n\n    remote_path = os.path.join(out_path, \"model.mar\")\n    print(f\"uploading to {remote_path}\")\n    fs, _, rpaths = fsspec.get_fs_token_paths(remote_path)\n    assert len(rpaths) == 1, \"must have single path\"\n    fs.put(mar_path, rpaths[0])\n\n\n# sphinx_gallery_thumbnail_path = '_static/img/gallery-lib.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}