{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/heads/master.tar.gz\n!tar xvf master.tar.gz torchx-master --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tiny ImageNet Model\n\nThis is a toy model for doing regression on the tiny imagenet dataset. It's used\nby the apps in the same folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path\nimport subprocess\nfrom typing import Tuple\n\nimport fsspec\nimport pytorch_lightning as pl\nimport torch\nimport torch.jit\nfrom torch.nn import functional as F\n\n\nclass TinyImageNetModel(pl.LightningModule):\n    \"\"\"\n    An very simple linear model for the tiny image net dataset.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.l1 = torch.nn.Linear(64 * 64, 4096)\n\n    # pyre-fixme[14]\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n    # pyre-fixme[14]\n    def training_step(\n        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_nb: int\n    ) -> torch.Tensor:\n        x, y = batch\n        loss = F.cross_entropy(self(x), y)\n        return loss\n\n    # pyre-fixme[3]: TODO(aivanou): Figure out why oss pyre can identify type but fb cannot.\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.02)\n\n\ndef export_inference_model(\n    model: TinyImageNetModel, out_path: str, tmpdir: str\n) -> None:\n    \"\"\"\n    export_inference_model uses TorchScript JIT to serialize the\n    TinyImageNetModel into a standalone file that can be used during inference.\n    TorchServe can also handle interpreted models with just the model.py file if\n    your model can't be JITed.\n    \"\"\"\n\n    print(\"exporting inference model\")\n    jit_path = os.path.join(tmpdir, \"model_jit.pt\")\n    jitted = torch.jit.script(model)\n    print(f\"saving JIT model to {jit_path}\")\n    torch.jit.save(jitted, jit_path)\n\n    model_name = \"tiny_image_net\"\n\n    mar_path = os.path.join(tmpdir, f\"{model_name}.mar\")\n    print(f\"creating model archive at {mar_path}\")\n    subprocess.run(\n        [\n            \"torch-model-archiver\",\n            \"--model-name\",\n            \"tiny_image_net\",\n            \"--handler\",\n            \"lightning_classy_vision/handler/handler.py\",\n            \"--version\",\n            \"1\",\n            \"--serialized-file\",\n            jit_path,\n            \"--export-path\",\n            tmpdir,\n        ],\n        check=True,\n    )\n\n    remote_path = os.path.join(out_path, \"model.mar\")\n    print(f\"uploading to {remote_path}\")\n    fs, _, rpaths = fsspec.get_fs_token_paths(remote_path)\n    assert len(rpaths) == 1, \"must have single path\"\n    fs.put(mar_path, rpaths[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}