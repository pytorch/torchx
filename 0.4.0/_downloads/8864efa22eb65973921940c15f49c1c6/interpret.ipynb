{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torchx[kfp]\n!wget --no-clobber https://github.com/pytorch/torchx/archive/refs/tags/v0.4.0.tar.gz\n!tar xf v0.4.0.tar.gz --strip-components=1\n\nNOTEBOOK = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nModel Interpretability Example\n=============================================\n\nThis is an example TorchX app that uses captum to analyze inputs to for model\ninterpretability purposes. It consumes the trained model from the trainer app\nexample and the preprocessed examples from the datapreproc app example. The\noutput is a series of images with integrated gradient attributions overlayed on\nthem.\n\nSee https://captum.ai/tutorials/CIFAR_TorchVision_Interpret for more info on\nusing captum.\n\nUsage\n---------\n\nRuns this main module as a python process locally. The run below assumes\nthat the model has been trained using the usage instructions in\n``torchx/examples/apps/lightning/train.py``.\n\n.. code:: shell-session\n\n  $ torchx run -s local_cwd utils.python\n      --script ./lightning/interpret.py\n      --\n      --load_path /tmp/torchx/train/last.ckpt\n      --output_path /tmp/torchx/interpret\n\nUse an image viewer to visualize the ``*.png`` files generated under the ``output_path``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For local runs with TorchX's  ``utils.python`` built-in is effectively\n          equal to running the main module directly (e.g. ``python ./interpret.py``).\n          The benefit of using TorchX to launch simple single-process python programs\n          is to launch on remote schedulers by swapping out ``-s local_cwd`` to\n          a remote scheduler like kubernetes by specifying ``-s kubernetes``.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import argparse\nimport itertools\nimport os.path\nimport sys\nimport tempfile\nfrom typing import List\n\nimport fsspec\nimport torch\nfrom torchx.examples.apps.lightning.data import (\n    create_random_data,\n    download_data,\n    TinyImageNetDataModule,\n)\nfrom torchx.examples.apps.lightning.model import TinyImageNetModel\n\n\n# ensure data and module are on the path\nsys.path.append(\".\")\n\n\n# FIXME: captum must be imported after torch otherwise it causes python to crash\nif True:\n    import numpy as np\n    from captum.attr import IntegratedGradients, visualization as viz\n\n\ndef parse_args(argv: List[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"example TorchX captum app\")\n    parser.add_argument(\n        \"--load_path\",\n        type=str,\n        help=\"checkpoint path to load model weights from\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--data_path\",\n        type=str,\n        help=\"path to load the training data from, if not provided, random dataset will be created\",\n    )\n    parser.add_argument(\n        \"--output_path\",\n        type=str,\n        help=\"path to place analysis results\",\n        required=True,\n    )\n\n    return parser.parse_args(argv)\n\n\ndef convert_to_rgb(arr: torch.Tensor) -> np.ndarray:  # pyre-ignore[24]\n    \"\"\"\n    This converts the image from a torch tensor with size (1, 1, 64, 64) to\n    numpy array with size (64, 64, 3).\n    \"\"\"\n    out = arr.squeeze().swapaxes(0, 2)\n    assert out.shape == (64, 64, 3), \"invalid shape produced\"\n    return out.numpy()\n\n\ndef main(argv: List[str]) -> None:\n    with tempfile.TemporaryDirectory() as tmpdir:\n        args = parse_args(argv)\n\n        # Init our model\n        model = TinyImageNetModel()\n\n        print(f\"loading checkpoint: {args.load_path}...\")\n        model.load_from_checkpoint(checkpoint_path=args.load_path)\n\n        # Download and setup the data module\n        if not args.data_path:\n            data_path = os.path.join(tmpdir, \"data\")\n            os.makedirs(data_path)\n            create_random_data(data_path)\n        else:\n            data_path = download_data(args.data_path, tmpdir)\n        data = TinyImageNetDataModule(\n            data_dir=data_path,\n            batch_size=1,\n        )\n\n        ig = IntegratedGradients(model)\n\n        data.setup(\"test\")\n        dataloader = data.test_dataloader()\n\n        # process first 5 images\n        for i, (input, label) in enumerate(itertools.islice(dataloader, 5)):\n            print(f\"analyzing example {i}\")\n            # input = input.unsqueeze(dim=0)\n            model.zero_grad()\n            attr_ig, delta = ig.attribute(\n                input,\n                target=label,\n                baselines=input * 0,\n                return_convergence_delta=True,\n            )\n\n            if attr_ig.count_nonzero() == 0:\n                # Our toy model sometimes has no IG results.\n                print(\"skipping due to zero gradients\")\n                continue\n\n            fig, axis = viz.visualize_image_attr(\n                convert_to_rgb(attr_ig),\n                convert_to_rgb(input),\n                method=\"blended_heat_map\",\n                sign=\"all\",\n                show_colorbar=True,\n                title=\"Overlayed Integrated Gradients\",\n            )\n            out_path = os.path.join(args.output_path, f\"ig_{i}.png\")\n            print(f\"saving heatmap to {out_path}\")\n            with fsspec.open(out_path, \"wb\") as f:\n                fig.savefig(f)\n\n\nif __name__ == \"__main__\" and \"NOTEBOOK\" not in globals():\n    main(sys.argv[1:])\n\n\n# sphinx_gallery_thumbnail_path = '_static/img/gallery-app.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}